---
title: "New York City AQI Data Exploration"
output: html_notebook
---

### Environment Setup

```{r output=FALSE}
# Load Libraries
# library(usethis)
# library(devtools)
library(data.table)
library(tidyverse)
library(DataExplorer)
# library(filenamer)
```

### Dataset URLs

We have compiled a list of data sets we'd like to explore for this analysis. Each URL contains a query to filter down the records between Jan 1 2018 and Jan 1 2022, and in Manhattan borough or New York county, or New York City.

```{r}
#source("./api/nyc_dataset_urls.R")
```

### Download Data From URLs

Download the URLs onto disk, to avoid storing in R memory. Taxi records are >100+ million, so downloading on disk would be faster. You can see status of download in print comments.

```{r warning=FALSE}
#source("./api/nyc_dataset_downloads.R")
df_list <- readRDS("./api/processed/nyc_dataset_list.csv")
```

### Exploratory Data Analysis

```{r}
# Cleaning functions
medianNA <- function(x){ifelse(is.na(x),
                     median(x, na.rm = TRUE), x)}

meanNA <- function(x){ifelse(is.na(x),
                     mean(x, na.rm = TRUE), x)}
```

In the following script, we do the following:

1. Read downloaded data from JSON into tibble of features stored as tibbles.
2. Create EDA report for each feature tibble using `DataExplorer`.
3. Save EDA reports and tibble of features.

```{r}
#source("./eda/reportdata.R")
features <- readRDS("./data/processed/features.RDS")
tb_features <- tibble(fts = features$df$list[1:nrow(features)])
```

Let's retrieve our constants for the analysis.

```{r}
constants <- readRDS("./api/processed/nyc_constants.csv")
```

**Air Quality Data**

Cleaning air quality (pm2.5) data to remove empty columns, and add week #, month, year to dataset.

```{r output = FALSE}
pm25 <- fread("./data/raw/new-york, usa-air-quality.csv",
                 data.table = TRUE)

# Drop columns with more than 30% total NA values
cols <- profile_missing(pm25) %>% filter(pct_missing > 0.30)
pm25[, as.character(cols$feature) := NULL]

# Split datetime to retrieve week, month, year
pm25[, week := week(date)]
pm25 <- separate(pm25, "date", c("year", "month", "day"),
                 sep = "/") 
# Fix data types 
pm25[, `:=` (year = as.integer(year), month = as.integer(month), day = as.integer(day))]
pm25[,pm25 := as.numeric(pm25)]

# Select rows by temporal var
pm25 <- pm25[year %between% list(year(constants$TIME_START),
                                 year(constants$TIME_END) - 1)]

# Fill NA with median
pm25 <- pm25[, lapply(.SD, medianNA), .SDcols = c("pm25"), by = .(year,month,week)]

# Extract week, month, year for temporal
temporal <- pm25 %>% group_by(year,month, week) %>% 
              summarise(n = n()) %>% data.table()

fwrite(temporal, "./data/processed/temporal.csv", col.names = TRUE)

# Get weekly pm25 data
pm25 <- pm25 %>% group_by(year,week) %>% 
              summarise(pm25 = mean(pm25)) %>% data.table()

fwrite(pm25, "./data/processed/pm25.csv", col.names = TRUE)
```


**Weather Data**

Cleaning weather data to remove empty columns, and add week #, month, year to dataset.

```{r}
weather <- fread("./data/raw/weather_2018_2022.csv",
                 data.table = TRUE, drop = c("source"))
```

```{r warning=FALSE}
# Split datetime into week, month, year
weather$week <- week(weather$datetime)
weather <- separate(weather, "datetime", c("year", "month", "day"), sep = "-")

# Split datetime into primary and secondary conditions
weather <- separate(weather, "conditions", 
                    c("primarycondition", 
                      "secondarycondition"), 
                    sep = ", ")
# Fill NAs for new columns
weather[is.na(secondarycondition), ]$secondarycondition <- "None"
```

```{r}
# Select rows by temporal var
weather <- weather[year < year(constants$TIME_END)]

# Fix data types
weather[, `:=`(year = as.integer(year), 
               month = as.integer(month), 
               week = as.integer(week))]

# Drop all NA and irrelevant columns 
weather <- weather %>% discard(~all(is.na(.))) 
weather[, day := NULL]
weather[, uvindex := NULL]

# Select useful numeric data
store <- weather[,.(year, week, name, latitude, longitude, description, stations)]

use <- setdiff(names(weather), c("name", "latitude",
                                 "longitude", "description",
                                 "stations", "month"))
weather <- weather %>% select(use)

# Fill NAs with median of (week, year)
weather <- weather[, lapply(.SD, medianNA), 
                   .SDcols = setdiff(use,c("year", "week")), 
                   by = .(year, week)]

# Missing data : windgust
profile_missing(weather) %>% filter(pct_missing > 0)

# Fill NA of missing (week, year) with median of (week-1, year)
weather[is.na(windgust),]$windgust <- weather[year == weather[is.na(windgust),]$year[1]][week == weather[is.na(windgust),]$week[1] - 1][, windgust] %>% median()
```
```{r}
### WE DID NOT INCLUDE THESE COLUMNS IN THE FINAL PM25 DATASET ###

# Convert conditions to factors
weather[,`:=`(primarycondition = as.factor(primarycondition), secondarycondition = as.factor(secondarycondition))]

weather_factors <- stack(data.frame("Clear" = 1, 
                                    "Partially cloudy" = 2,"Overcast" = 3,
                                    "Rain" = 4, "Snow" = 5, "None" = 0))

# Save weather factor codes for future reference
saveRDS(weather_factors, "./data/processed/weather_factors.csv")

# Map factors to codes
weather$primarycondition <- weather$primarycondition %>% 
                              recode_factor("Clear" = 1, 
                                            "Partially cloudy" = 2,
                                            "Overcast" = 3, 
                                            "Rain" = 4, "Snow" = 5, 
                                            "None" = 0)

weather$secondarycondition <- weather$secondarycondition %>% 
                              recode_factor("Clear" = 1, 
                                            "Partially cloudy" = 2,
                                            "Overcast" = 3, 
                                            "Rain" = 4, "Snow" = 5, 
                                            "None" = 0)
```

```{r}
# Get weekly weather data
weather <- weather[,lapply(.SD, mean), 
                   .SDcols = setdiff(use,c("year", "week",
                                           "primarycondition",
                                           "secondarycondition")), by = .(year, week)]

fwrite(weather, "./data/processed/weather.csv")
```



### Debugging

Problem : Same week repeating for different months, because week is parsed from datetime. 

What I want: Map week to months to merge monthly data with weekly data. 

```{r}
# save temporal vars
temporal <- fread("./data/processed/temporal.csv")
```

We need around 212 - 243 rows in the merged dataset.

```{r}
# Read data
taxi_monthly <- fread("./data/raw/taxi/data_reports_monthly.csv", data.table = TRUE)

# Split datetime to retrieve week, month, year
taxi_monthly <- separate(taxi_monthly, "Month/Year", c("year", "month"), sep = "-") 

# Fix data types 
taxi_monthly[, `:=`(year = as.integer(year), month = 
as.integer(month))]

# Select rows by temporal var
taxi_monthly <- taxi_monthly[year %between% list(year(constants$TIME_START),year(constants$TIME_END) - 1)]

# Sort temporal and taxi_monthly BOTH by (year, month)
taxi_monthly <- taxi_monthly %>% group_by(month, year) %>% arrange(year, month)
```

```{r}
# Merge datasets to get (54 * 4 * 6) rows + (54*3) repeating rows
merge <- merge(taxi_monthly, temporal, sort = FALSE) %>% data.table() # 1458 rows

# Remove month and n
merge <- merge[, c("month", "n") := NULL]
```

*PROBLEM FIXED -> # rows = 1458*

**Taxi Weekly Data**

Cleaning weekly taxi data to remove empty columns, and fix datatypes.

```{r}
# Assign merged dataset as weekly taxi data
taxi_weekly <- merge

# Remove NA and irrelevant columns
taxi_weekly[taxi_weekly == '-'] <- NA

# Drop columns with more than 30% total NA values
cols <- profile_missing(taxi_weekly) %>% filter(pct_missing < 0.30)
taxi_weekly <- taxi_weekly %>% select(cols$feature) %>% tibble() %>% relocate(week, .after = year) # change week column's location

taxi_weekly <- data.table(taxi_weekly)

names(taxi_weekly)
str(taxi_weekly)
gsub(c(" ", ","),"", noquote(taxi_weekly$`Trips Per Day`))

# Rearrange columns for easy joins
t <- taxi_weekly %>% arrange(year, week) %>% group_by(year, week, `License Class`) %>% data.table()

t
#t[, lapply(.SD, mean), by = .(year, week, `License Class`)]
```


### Combining Datasets

1. Combine *weather* and *PM2.5* datasets.

```{r}
# Read Data
weather <- fread("./data/processed/weather.csv")
pm25 <- fread("./data/processed/pm25.csv")

# Set keys
setkey(pm25, year, week)
setkey(weather, year, week)

# Merge
master <- weather[pm25]
```

### AQI Data

UNDER PROGRESS.

```{r}
### To use New York's Air Quality System (AQS), we set up a user account.
# Store API user emails, tokens, passwords
soc_token <- Sys.getenv("SOCRATA_TOKEN")
soc_email <- Sys.getenv("SOCRATA_EMAIL")

soc_pass <- Sys.getenv("SOCRATA_PASSWORD")
aqs_key <- Sys.getenv("AQS_KEY")
aqs_email <- Sys.getenv("AQS_EMAIL")

# Create Users
aqs_user <- create_user(email=aqs_email,
                        key=aqs_key)
```

```{r}
# New York County Identifiers
identifiers <- list()
state_fips <- data.table(aqs_list_states(aqs_user))
identifiers$ny_fips_code <- state_fips[value_represented == constants$STATE]$code
identifiers$ny_counties <- data.table(aqs_list_counties(aqs_user, state=identifiers$ny_fips_code))
identifiers$nyCounty_fips_code <- identifiers$ny_counties[value_represented == constants$COUNTY]$code

```

```{r}
library(aqsr)
aqi_pollutants <- aqs_list_parameters(aqs_user, pc="AQI POLLUTANTS")
aqi_forecast <- aqs_list_parameters(aqs_user, pc="FORECAST")
```

```{r}
# New York County Identifiers
identifiers <- list()
state_fips <- data.table(aqs_list_states(aqs_user))
identifiers$ny_fips_code <- state_fips[value_represented == constants$STATE]$code
identifiers$ny_counties <- data.table(aqs_list_counties(aqs_user, state=identifiers$ny_fips_code))
identifiers$nyCounty_fips_code <- identifiers$ny_counties[value_represented == constants$COUNTY]$code

s2 <- aqs_dailyData(aqs_user=aqs_user,
                     endpoint="byCounty",
                     state=identifiers$ny_fips_code,
                     county=identifiers$nyCounty_fips_code,
                     bdate="20210101",
                     edate="20211231",
                     param=aqi_pollutants$code[6])

s2
```

```{r}
# Set unique keys
setkey(temporal, week, month, year)
setkey(taxi_monthly, month, year)

# Arrange datasets
uniqueN(temporal)

pm25 %>% arrange(year, week)
temporal %>% arrange(year, week)
cbind(temporal$week, pm25$week)
```

### Road Networks 
```{r}
# Read this shape file with the rgdal library. 
library(rgdal)

my_spdf <- readOGR( 
  dsn= paste0(getwd(), "/data/citymap_citymap_v1") , 
  layer="citymap_citymap_v1",
  verbose=FALSE)
```

```{r}
# Basic plot of this shape file:
par(mar=c(0,0,0,0))
plot(my_spdf, col="#f2f2f2", bg="black", lwd=0.25 )
```

```{r}
head(my_spdf)
```

```{r}
nyC_map <- my_spdf[my_spdf$boro_nm == BOROUGH]

is_(my_spdf$boro_nm == BOROUGH )

str(my_spdf)

names(my_spdf)

unique(my_spdf$ownership_)

my_spdf

# Fix Data Types
# taxi_weekly[, `:=`("`Trips Per Day`" = as.integer("`Trips Per Day`"))]
# 
#           "Unique Drivers"= as.integer("Unique Drivers"),
#            "Unique Vehicles" = as.integer("Unique Vehicles"),
#           "Vehicles Per Day" = as.integer("Vehicles Per Day"),
#           "Avg Minutes Per Trip" = as.numeric("Avg Minutes Per Trip"))]
```


```{r}
# Basic plot of this shape file:
par(mar=c(0,0,0,0))
plot(nyC_map, col="#f2f2f2", bg="black", lwd=0.25 )
```
