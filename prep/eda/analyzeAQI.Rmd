---
title: "New York City AQI Data Exploration"
output: html_notebook
---

### Environment Setup

```{r warning=FALSE}
# Load Libraries
library(RSocrata)
library(aqsr)
library(usethis)
library(devtools)
library(data.table)
library(tidyverse)
library(DataExplorer)
library(filenamer)
library(validate)
library(request)
library(downloader)
```

### Dataset URLs

We have compiled a list of data sets we'd like to explore for this analysis. Each URL contains a query to filter down the records between Jan 1 2018 and Jan 1 2022, and in Manhattan borough or New York county, or New York City.

```{r}
#source("./api/nyc_dataset_urls.R")
```

### Download Data From URLs

Download the URLs onto disk, to avoid storing in R memory. Taxi records are >100+ million, so downloading on disk would be faster. You can see status of download in print comments.

```{r warning=FALSE}
#source("./api/nyc_dataset_downloads.R")
df_list <- readRDS("./api/processed/nyc_dataset_list.csv")
```

### Exploratory Data Analysis

```{r}
# Cleaning functions
medianNA <- function(x){ifelse(is.na(x),
                     median(x, na.rm = TRUE), x)}

meanNA <- function(x){ifelse(is.na(x),
                     mean(x, na.rm = TRUE), x)}
```

In the following script, we do the following:

1. Read downloaded data from JSON into tibble of features stored as tibbles.
2. Create EDA report for each feature tibble using `DataExplorer`.
3. Save EDA reports and tibble of features.

```{r}
#source("./eda/reportdata.R")
features <- readRDS("./data/processed/features.RDS")
tb_features <- tibble(fts = features$df$list[1:nrow(features)])
```

Let's retrieve our constants for the analysis.

```{r}
constants <- readRDS("./api/processed/nyc_constants.csv")
```

**Weather Data**

Cleaning weather data to remove empty columns, and add week #, month, year to dataset.

```{r}
weather <- fread("./data/raw/weather_2018_2022.csv",
                 data.table = TRUE, drop = c("source"))
```

```{r}
# Split datetime into week, month, year
weather$week <- week(weather$datetime)
weather <- separate(weather, "datetime", c("year", "month", "day"), sep = "-")

# Split datetime into primary and secondary conditions
weather <- separate(weather, "conditions", 
                    c("primarycondition", 
                      "secondarycondition"), 
                    sep = ", ")
# Fill NAs for new columns
weather[is.na(secondarycondition), ]$secondarycondition <- "None"
```

```{r}
# Select rows by temporal var
weather <- weather[year < year(constants$TIME_END)]

# Fix data types
weather[, `:=`(year = as.integer(year), 
               month = as.integer(month), 
               week = as.integer(week))]

# Drop all NA and irrelevant columns 
weather <- weather %>% discard(~all(is.na(.))) 
weather[, day := NULL]
weather[, uvindex := NULL]

# Select useful numeric data
store <- weather[,.(year, week, name, latitude, longitude, description, stations)]

use <- setdiff(names(weather), c("name", "latitude",
                                 "longitude", "description",
                                 "stations", "month"))
weather <- weather %>% select(use)

# Fill NAs with median of (week, year)
weather <- weather[, lapply(.SD, medianNA), 
                   .SDcols = setdiff(use,c("year", "week")), 
                   by = .(year, week)]

# Missing data : windgust
profile_missing(weather) %>% filter(pct_missing > 0)

# Fill NA of missing (week, year) with median of (week-1, year)
weather[is.na(windgust),]$windgust <- weather[year == weather[is.na(windgust),]$year[1]][week == weather[is.na(windgust),]$week[1] - 1][, windgust] %>% median()
```
```{r}
weather[,`:=`(primarycondition = as.factor(primarycondition), secondarycondition = as.factor(secondarycondition))]

weather_factors <- stack(data.frame("Clear" = 1, 
                                    "Partially cloudy" = 2,"Overcast" = 3,
                                    "Rain" = 4, "Snow" = 5, "None" = 0))

saveRDS(weather_factors, "./data/processed/weather_factors.csv")

weather$primarycondition <- weather$primarycondition %>% 
                              recode_factor("Clear" = 1, 
                                            "Partially cloudy" = 2,
                                            "Overcast" = 3, 
                                            "Rain" = 4, "Snow" = 5, 
                                            "None" = 0)

weather$secondarycondition <- weather$secondarycondition %>% 
                              recode_factor("Clear" = 1, 
                                            "Partially cloudy" = 2,
                                            "Overcast" = 3, 
                                            "Rain" = 4, "Snow" = 5, 
                                            "None" = 0)
```

```{r}
# Get weekly weather data
use_weather <- weather[,lapply(.SD, mean), 
                   .SDcols = setdiff(use,c("year", "week",
                                           "primarycondition",
                                           "secondarycondition")), 
                   by = .(year, week)]

fwrite(use_weather, "./data/processed/weather.csv")
```


**Air Quality Data**

```{r}
pm25 <- fread("./data/raw/new-york, usa-air-quality.csv",
                 data.table = TRUE)

# Drop columns with more than 30% total NA values
cols <- profile_missing(pm25) %>% filter(pct_missing > 0.30)
pm25[, as.character(cols$feature) := NULL]

# Split datetime to retrieve week, month, year
pm25[, week := week(date)]
pm25 <- separate(pm25, "date", c("year", "month", "day"),
                 sep = "/") 
# Fix data types 
pm25[,lapply(.SD, as.integer),.SDcols = c("year", "month", "day")]
pm25[,pm25 := as.numeric(pm25)]

# Select rows by temporal var
pm25 <- pm25[year %between% list(year(constants$TIME_START),
                                 year(constants$TIME_END) - 1)]

# Fill NA with median
pm25 <- pm25[, lapply(.SD, medianNA), .SDcols = c("pm25"), by = .(week, year)]

# Get weekly pm25 data
pm25 <- pm25 %>% group_by(week, year) %>% 
              summarise(avg_pm25 = mean(pm25), n = n()) %>% data.table()

fwrite(pm25, "./data/processed/pm25.RDS", col.names = TRUE)
```

Combine *weather* and *PM2.5* datasets.

```{r}
names(pm25)
nrow(use_weather)
setkey(pm25, .(year, week))

# 
# names(pm25)
# names(use_weather)
# pm25[use_weather, on = .(week, year)]
```

```{r}
features <- profile_missing(data) %>% filter(pct_missing < 0.35)
features$feature
```

### AQI Data
```{r}
aqi_pollutants <- aqs_list_parameters(aqs_user, pc="AQI POLLUTANTS")
aqi_forecast <- aqs_list_parameters(aqs_user, pc="FORECAST")
```

```{r}
s2 <- aqs_dailyData(aqs_user=aqs_user,
                     endpoint="byCounty",
                     state=ny_fips_code,
                     county=nyCounty_fips_code,
                     bdate="20160101",
                     edate="20161231",
                     param=aqi_pollutants$code[1])
```

### Road Networks 
```{r}
# Read this shape file with the rgdal library. 
library(rgdal)

my_spdf <- readOGR( 
  dsn= paste0(getwd(), "/data/citymap_citymap_v1") , 
  layer="citymap_citymap_v1",
  verbose=FALSE)
```

```{r}
# Basic plot of this shape file:
par(mar=c(0,0,0,0))
plot(my_spdf, col="#f2f2f2", bg="black", lwd=0.25 )
```

```{r}
head(my_spdf)
```

```{r}
nyC_map <- my_spdf[my_spdf$boro_nm == BOROUGH]

is_(my_spdf$boro_nm == BOROUGH )

str(my_spdf)

names(my_spdf)

unique(my_spdf$ownership_)

my_spdf
```


```{r}
# Basic plot of this shape file:
par(mar=c(0,0,0,0))
plot(nyC_map, col="#f2f2f2", bg="black", lwd=0.25 )
```
