---
title: "New York City AQI Data Exploration"
output: html_notebook
---

### Environment Setup

```{r output=FALSE}
# Load Libraries
# library(usethis)
# library(devtools)
library(data.table)
library(tidyverse)
library(DataExplorer)
# library(filenamer)
```

### Dataset URLs

We have compiled a list of data sets we'd like to explore for this analysis. Each URL contains a query to filter down the records between Jan 1 2018 and Jan 1 2022, and in Manhattan borough or New York county, or New York City.

```{r}
#source("./api/nyc_dataset_urls.R")
```

### Download Data From URLs

Download the URLs onto disk, to avoid storing in R memory. Taxi records are >100+ million, so downloading on disk would be faster. You can see status of download in print comments.

```{r warning=FALSE}
#source("./api/nyc_dataset_downloads.R")
df_list <- readRDS("./api/processed/nyc_dataset_list.csv")
```

### Exploratory Data Analysis

```{r}
# Cleaning functions
medianNA <- function(x){ifelse(is.na(x),
                     median(x, na.rm = TRUE), x)}

meanNA <- function(x){ifelse(is.na(x),
                     mean(x, na.rm = TRUE), x)}
```

In the following script, we do the following:

1. Read downloaded data from JSON into tibble of features stored as tibbles.
2. Create EDA report for each feature tibble using `DataExplorer`.
3. Save EDA reports and tibble of features.

```{r}
#source("./eda/reportdata.R")
features <- readRDS("./data/processed/features.RDS")
tb_features <- tibble(fts = features$df$list[1:nrow(features)])
```

Let's retrieve our constants for the analysis.

```{r}
constants <- readRDS("./api/processed/nyc_constants.csv")
```

**Weather Data**

Cleaning weather data to remove empty columns, and add week #, month, year to dataset.

```{r}
weather <- fread("./data/raw/weather_2018_2022.csv",
                 data.table = TRUE, drop = c("source"))
```

```{r warning=FALSE}
# Split datetime into week, month, year
weather$week <- week(weather$datetime)
weather <- separate(weather, "datetime", c("year", "month", "day"), sep = "-")

# Split datetime into primary and secondary conditions
weather <- separate(weather, "conditions", 
                    c("primarycondition", 
                      "secondarycondition"), 
                    sep = ", ")
# Fill NAs for new columns
weather[is.na(secondarycondition), ]$secondarycondition <- "None"
```

```{r}
# Select rows by temporal var
weather <- weather[year < year(constants$TIME_END)]

# Fix data types
weather[, `:=`(year = as.integer(year), 
               month = as.integer(month), 
               week = as.integer(week))]

# Drop all NA and irrelevant columns 
weather <- weather %>% discard(~all(is.na(.))) 
weather[, day := NULL]
weather[, uvindex := NULL]

# Select useful numeric data
store <- weather[,.(year, week, name, latitude, longitude, description, stations)]

use <- setdiff(names(weather), c("name", "latitude",
                                 "longitude", "description",
                                 "stations", "month"))
weather <- weather %>% select(use)

# Fill NAs with median of (week, year)
weather <- weather[, lapply(.SD, medianNA), 
                   .SDcols = setdiff(use,c("year", "week")), 
                   by = .(year, week)]

# Missing data : windgust
profile_missing(weather) %>% filter(pct_missing > 0)

# Fill NA of missing (week, year) with median of (week-1, year)
weather[is.na(windgust),]$windgust <- weather[year == weather[is.na(windgust),]$year[1]][week == weather[is.na(windgust),]$week[1] - 1][, windgust] %>% median()
```
```{r}
weather[,`:=`(primarycondition = as.factor(primarycondition), secondarycondition = as.factor(secondarycondition))]

weather_factors <- stack(data.frame("Clear" = 1, 
                                    "Partially cloudy" = 2,"Overcast" = 3,
                                    "Rain" = 4, "Snow" = 5, "None" = 0))

saveRDS(weather_factors, "./data/processed/weather_factors.csv")

weather$primarycondition <- weather$primarycondition %>% 
                              recode_factor("Clear" = 1, 
                                            "Partially cloudy" = 2,
                                            "Overcast" = 3, 
                                            "Rain" = 4, "Snow" = 5, 
                                            "None" = 0)

weather$secondarycondition <- weather$secondarycondition %>% 
                              recode_factor("Clear" = 1, 
                                            "Partially cloudy" = 2,
                                            "Overcast" = 3, 
                                            "Rain" = 4, "Snow" = 5, 
                                            "None" = 0)
```

```{r}
# Get weekly weather data
weather <- weather[,lapply(.SD, mean), 
                   .SDcols = setdiff(use,c("year", "week",
                                           "primarycondition",
                                           "secondarycondition")), 
                   by = .(year, week)]

fwrite(weather, "./data/processed/weather.csv")
```


**Air Quality Data**

```{r output = FALSE}
pm25 <- fread("./data/raw/new-york, usa-air-quality.csv",
                 data.table = TRUE)

# Drop columns with more than 30% total NA values
cols <- profile_missing(pm25) %>% filter(pct_missing > 0.30)
pm25[, as.character(cols$feature) := NULL]

# Split datetime to retrieve week, month, year
pm25[, week := week(date)]
pm25 <- separate(pm25, "date", c("year", "month", "day"),
                 sep = "/") 
# Fix data types 
pm25[, `:=` (year = as.integer(year), month = as.integer(month), day = as.integer(day))]
pm25[,pm25 := as.numeric(pm25)]

# Select rows by temporal var
pm25 <- pm25[year %between% list(year(constants$TIME_START),
                                 year(constants$TIME_END) - 1)]

# Fill NA with median
pm25 <- pm25[, lapply(.SD, medianNA), .SDcols = c("pm25"), by = .(year,month,week)]

# Extract week, month, year for temporal
temporal <- pm25 %>% group_by(year,month, week) %>% 
              summarise(n = n()) %>% data.table()

fwrite(temporal, "./data/processed/temporal.csv", col.names = TRUE)

# Get weekly pm25 data
pm25 <- pm25 %>% group_by(year,week) %>% 
              summarise(pm25 = mean(pm25)) %>% data.table()

fwrite(pm25, "./data/processed/pm25.csv", col.names = TRUE)
```

### Debugging
Problem : Same week repeating for different months, because week is parsed from datetime. 
What I want: Map week to months to merge monthly data with weekly data. 

```{r}
# save temporal
temporal <- fread("./data/processed/temporal.csv")
```
We need around 212 - 243 rows in the merged dataset.

```{r}
taxi_monthly <- fread("./data/raw/taxi/data_reports_monthly.csv", data.table = TRUE)

# Split datetime to retrieve week, month, year
taxi_monthly <- separate(taxi_monthly, "Month/Year", c("year", "month"), sep = "-") 

# Fix data types 
taxi_monthly[, `:=`(year = as.integer(year), month = 
as.integer(month))]

# Select rows by temporal var
taxi_monthly <- taxi_monthly[year %between% list(year(constants$TIME_START),year(constants$TIME_END) - 1)]

# Sort temporal and taxi_monthly BOTH by (year, month)
taxi_monthly <- taxi_monthly %>% group_by(month, year) %>% arrange(year, month)

# Merge datasets to get (54 * 4 * 6) rows + (54*3) repeating rows
merge <- merge(taxi_monthly, temporal, sort = FALSE) %>% data.table()# 1458 rows

# remove month and n
merge <- merge[, c("month", "n") := NULL]

taxi_weekly <- merge

# Remove NA and irrelevant columns
taxi_weekly[taxi_weekly == '-'] <- NA

cols <- profile_missing(taxi_weekly) %>% filter(pct_missing < 0.30)

taxi_weekly <- taxi_weekly %>% select(cols$feature) %>% tibble() %>% relocate(week, .after = year)
taxi_weekly <- data.table(taxi_weekly)



# Fix Data Types
# taxi_weekly[, `:=`("`Trips Per Day`" = as.integer("`Trips Per Day`"))]
# 
#           "Unique Drivers"= as.integer("Unique Drivers"),
#            "Unique Vehicles" = as.integer("Unique Vehicles"),
#           "Vehicles Per Day" = as.integer("Vehicles Per Day"),
#           "Avg Minutes Per Trip" = as.numeric("Avg Minutes Per Trip"))]

names(taxi_weekly)
str(taxi_weekly)
gsub(c(" ", ","),"", noquote(taxi_weekly$`Trips Per Day`))

# Rearrange columns for easy joins

t <- taxi_weekly %>% arrange(year, week) %>% group_by(year, week, `License Class`) %>% data.table()

t

#t[, lapply(.SD, mean), by = .(year, week, `License Class`)]
```

```{r}
# Set unique keys
setkey(temporal, week, month, year)
setkey(taxi_monthly, month, year)

# Arrange datasets

uniqueN(temporal)

temporal

pm25 %>% arrange(year, week)
temporal %>% arrange(year, week)
cbind(temporal$week, pm25$week)
```

Combine *weather* and *PM2.5* datasets.

```{r}
weather <- fread("./data/processed/weather.csv")
pm25 <- fread("./data/processed/pm25.csv")

setkey(pm25, year, week)
setkey(weather, year, week)

master <- weather[pm25]


```


```{r}
# # Read data
fts6 <- data.frame(tb_features$fts[6][1])
fts6$ftrName <- features$names[6]

# for (i in 1:nrow(features)) {
#   
#   fts6 <- data.frame(tb_features$fts[i][1])
#   fts6$feature_name <- features$names[i]
#   
# }



features$names

# Drop columns with more than 30% total NA values
cols <- profile_missing(fts2) %>% filter(pct_missing < 0.30)
fts2 <- fts2[, cols$feature] %>% data.table()

names(fts2)
# Split datetime to retrieve week, month, year
fts2[, week := week(occured_on)]
fts2 <- separate(fts2, "occured_on", c("year", "month", "day"),
                 sep = "-")
```


























```{r}
# # Read data
# fts2 <- data.frame(tb_features$fts[2][1])
# fts2$ftrName <- features$names[2]
# 
# # Drop columns with more than 30% total NA values
# cols <- profile_missing(fts2) %>% filter(pct_missing < 0.30)
# fts2 <- fts2[, cols$feature] %>% data.table()
# 
# names(fts2)
# # Split datetime to retrieve week, month, year
# fts2[, week := week(occured_on)]
# fts2 <- separate(fts2, "occured_on", c("year", "month", "day"),
#                  sep = "-") 
# keep <- setdiff(names(fts2), c("day", "bus_no", "bus_company_name",  
#                                "month", "incident_number", "created_on",
#                                "informed_on",
#                                "school_age_or_prek","last_updated_on", "boro"))
# fts2 <- fts2 %>% select(keep)
# 
# fts2[,lapply(.SD,uniqueN)]
```











### AQI Data
```{r}
aqi_pollutants <- aqs_list_parameters(aqs_user, pc="AQI POLLUTANTS")
aqi_forecast <- aqs_list_parameters(aqs_user, pc="FORECAST")
```

```{r}
s2 <- aqs_dailyData(aqs_user=aqs_user,
                     endpoint="byCounty",
                     state=ny_fips_code,
                     county=nyCounty_fips_code,
                     bdate="20160101",
                     edate="20161231",
                     param=aqi_pollutants$code[1])
```

### Road Networks 
```{r}
# Read this shape file with the rgdal library. 
library(rgdal)

my_spdf <- readOGR( 
  dsn= paste0(getwd(), "/data/citymap_citymap_v1") , 
  layer="citymap_citymap_v1",
  verbose=FALSE)
```

```{r}
# Basic plot of this shape file:
par(mar=c(0,0,0,0))
plot(my_spdf, col="#f2f2f2", bg="black", lwd=0.25 )
```

```{r}
head(my_spdf)
```

```{r}
nyC_map <- my_spdf[my_spdf$boro_nm == BOROUGH]

is_(my_spdf$boro_nm == BOROUGH )

str(my_spdf)

names(my_spdf)

unique(my_spdf$ownership_)

my_spdf
```


```{r}
# Basic plot of this shape file:
par(mar=c(0,0,0,0))
plot(nyC_map, col="#f2f2f2", bg="black", lwd=0.25 )
```
